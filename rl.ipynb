{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Queue\n",
    "import random\n",
    "from random import shuffle, randint\n",
    "from threading import Thread\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import cPickle as pkl\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string(\"model\", \"\", \"The name of model runned.\")\n",
    "tf.app.flags.DEFINE_string(\"data_path\", \"\", \"Path expression to data file.\")\n",
    "tf.app.flags.DEFINE_string(\"input_vocab\", \"\", \"Path to input vocabulary file.\")\n",
    "tf.app.flags.DEFINE_string(\"output_vocab\", \"\", \"Path to output vocabulary file.\")\n",
    "tf.app.flags.DEFINE_integer(\"input_vsize\", 0,\"Number of words in input vocabulary.\")\n",
    "tf.app.flags.DEFINE_integer(\"output_vsize\", 0,\"Number of words in output vocabulary.\")\n",
    "tf.app.flags.DEFINE_string(\"ckpt_root\", \"\", \"Directory for checkpoint root.\")\n",
    "tf.app.flags.DEFINE_string(\"summary_dir\", \"\", \"Directory for summary files.\")\n",
    "tf.app.flags.DEFINE_string(\"mode\", \"train\", \"train/decode mode\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 1, \"Size of minibatch. Beam size in decode mode.\")\n",
    "# ----------- Train mode related flags ------------------\n",
    "tf.app.flags.DEFINE_float(\"lr\", 0.15, \"Initial learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"min_lr\", 0.01, \"Minimum learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"max_grad_norm\", 1.0, \"Maximum gradient norm for gradient clipping.\")\n",
    "tf.app.flags.DEFINE_integer(\"decay_step\", 30000, \"Exponential decay step.\")\n",
    "tf.app.flags.DEFINE_float(\"decay_rate\", 0.1, \"Exponential decay rate.\")\n",
    "tf.app.flags.DEFINE_integer(\"max_run_steps\", 1000000, \"Maximum number of run steps.\")\n",
    "tf.app.flags.DEFINE_float(\"dropout\", 0.0, \"Dropout rate.\")\n",
    "tf.app.flags.DEFINE_string(\"valid_path\", \"\", \"Path expression to validation set.\")\n",
    "tf.app.flags.DEFINE_integer(\"valid_freq\", 1000, \"How often to run eval.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_valid_batch\", 30, \"Number valid batches in each _Valid step.\")\n",
    "tf.app.flags.DEFINE_integer(\"checkpoint_secs\", 1200, \"How often to checkpoint.\")\n",
    "tf.app.flags.DEFINE_integer(\"max_to_keep\", None,\n",
    "                            \"Maximum number of checkpoints to keep. \"\n",
    "                            \"Keep all by default\")\n",
    "tf.app.flags.DEFINE_integer(\"display_freq\", 200, \"How often to print.\")\n",
    "tf.app.flags.DEFINE_integer(\"verbosity\", 20,\n",
    "                            \"tf.logging verbosity (default INFO).\")\n",
    "# ----------- Data reading related flags ------------------\n",
    "tf.app.flags.DEFINE_bool(\"use_bucketing\", False,\n",
    "                         \"Whether bucket articles of similar length.\")\n",
    "tf.app.flags.DEFINE_bool(\"truncate_input\", False,\n",
    "                         \"Truncate inputs that are too long. If False, \"\n",
    "                         \"examples that are too long are discarded.\")\n",
    "# ----------- Decode mode related flags ------------------\n",
    "tf.app.flags.DEFINE_string(\"decode_dir\", \"\", \"Directory for decode summaries.\")\n",
    "tf.app.flags.DEFINE_integer(\"extract_topk\", 3,\n",
    "                            \"Number of sentence extracted in decode mode.\")\n",
    "# ----------- general flags ----------------\n",
    "tf.app.flags.DEFINE_integer(\"emb_dim\", 128, \"Dim of word embedding.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_gpus\", 1, \"Number of gpus used.\")\n",
    "# ----------- summarunner related flags ----------------\n",
    "tf.app.flags.DEFINE_integer(\"enc_layers\", 1, \"Number of encoder layers.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_sents_doc\", 100,\n",
    "                            \"Maximum number of sentences in a document.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_words_sent\", 50,\n",
    "                            \"Maximum number of words in a sentence.\")\n",
    "tf.app.flags.DEFINE_integer(\"rel_pos_max_idx\", 11,\n",
    "                            \"Maximum index of relative position embedding.\")\n",
    "tf.app.flags.DEFINE_integer(\"enc_num_hidden\", 512,\n",
    "                            \"Number of hidden units in encoder RNN.\")\n",
    "tf.app.flags.DEFINE_integer(\"pos_emb_dim\", 64,\n",
    "                            \"Dimension of positional embedding.\")\n",
    "tf.app.flags.DEFINE_integer(\"doc_repr_dim\", 512,\n",
    "                            \"Dimension of document representation.\")\n",
    "tf.app.flags.DEFINE_string(\"word_conv_k_sizes\", \"3,5,7\",\n",
    "                           \"Kernel sizes of word-level CNN.\")\n",
    "tf.app.flags.DEFINE_integer(\"word_conv_filter\", 128,\n",
    "                            \"Number of output filters of all kernel sizes.\")\n",
    "tf.app.flags.DEFINE_integer(\"min_num_input_sents\", 0,\n",
    "                            \"Minimum number of sentences in input docuement.\")\n",
    "tf.app.flags.DEFINE_integer(\"min_num_words_sent\", 0,\n",
    "                            \"Ignore sentences shorter than this threshold.\")\n",
    "tf.app.flags.DEFINE_integer(\"trg_weight_norm\", 0,\n",
    "                            \"Normalize the extraction target weights. \"\n",
    "                            \"No normalization if it is not positive.\")\n",
    "# ----------- summarunner_rf related flags ----------------\n",
    "tf.app.flags.DEFINE_string(\"train_mode\", \"sl\",\n",
    "                           \"Kernel sizes of word-level CNN.\")\n",
    "tf.app.flags.DEFINE_string(\"mlp_num_hiddens\", \"256\", \"Kernel sizes of word-level CNN.\")\n",
    "tf.app.flags.DEFINE_float(\"rl_coef\", 1.0, \"Coefficient for RL loss in SL+RL mode.\")  # Hệ số mất mát\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, word2id):\n",
    "        self.word2id = word2id\n",
    "        self.id2word = {v: k for k, v in word2id.items()}\n",
    "        assert len(self.word2id) == len(self.id2word)\n",
    "        self.PAD_IDX = 0\n",
    "        self.UNK_IDX = 1\n",
    "        self.PAD_TOKEN = 'PAD_TOKEN'\n",
    "        self.UNK_TOKEN = 'UNK_TOKEN'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def i2w(self, idx):\n",
    "        return self.id2word[idx]\n",
    "\n",
    "    def w2i(self, w):\n",
    "        if w in self.word2id:\n",
    "            return self.word2id[w]\n",
    "        else:\n",
    "            return self.UNK_IDX\n",
    "\n",
    "    def get_ids(self, text):\n",
    "        \"\"\"Get ids corresponding to words in text.\n",
    "    Assumes tokens separated by space.\n",
    "\n",
    "    Args:\n",
    "      text: a string with tokens separated by space.\n",
    "\n",
    "    Returns:\n",
    "      A list of ints representing word ids.\n",
    "    \"\"\"\n",
    "        return [self.w2i(w) for w in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/word2id.json\") as f:\n",
    "    word2id = json.load(f)\n",
    "    print(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input= Vocab(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_input.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
